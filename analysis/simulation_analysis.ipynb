{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Analysis"
   ],
   "id": "d25ab75d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pingouin as pg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pingouin\")\n",
    "sns.set_theme(style=\"whitegrid\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "bdd3e985"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ],
   "id": "b50040f8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Paths ---\n",
    "SINGLE_TASKS_DIR = Path(\"../data/model/single_tasks\")\n",
    "TYPING_SIM_DIR = SINGLE_TASKS_DIR / \"typing_simulation\"\n",
    "NBACK_SIM_DIR = SINGLE_TASKS_DIR / \"nback_simulation\"\n",
    "\n",
    "SIM_DATA_DIR = Path(\"../data/model/simulated_participants_preprocessed\")\n",
    "HUMAN_DATA_DIR = Path(\"../data/experiment/data_preprocessed\")\n",
    "\n",
    "# --- Color palette ---\n",
    "COLORS = {\n",
    "    \"primary_dark\": \"#000000\",\n",
    "    \"teal\": \"#4A8A94\",\n",
    "    \"pink\": \"#E5A3B8\",\n",
    "    \"white\": \"#FFFFFF\",\n",
    "    \"teal_dark\": \"#2E5A62\",\n",
    "    \"pink_dark\": \"#D47A96\",\n",
    "}\n",
    "\n",
    "CONDITION_COLORS = {\n",
    "    \"sequential\": COLORS[\"teal\"],\n",
    "    \"interrupted\": COLORS[\"pink\"],\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.color\": COLORS[\"primary_dark\"],\n",
    "    \"axes.labelcolor\": COLORS[\"primary_dark\"],\n",
    "    \"xtick.color\": COLORS[\"primary_dark\"],\n",
    "    \"ytick.color\": COLORS[\"primary_dark\"],\n",
    "    \"axes.edgecolor\": COLORS[\"primary_dark\"],\n",
    "    \"figure.facecolor\": COLORS[\"white\"],\n",
    "    \"axes.facecolor\": COLORS[\"white\"],\n",
    "})\n",
    "\n",
    "# --- Analysis constants ---\n",
    "CONDITION_ORDER = [\"Seq-Easy\", \"Seq-Hard\", \"Int-Easy\", \"Int-Hard\"]\n",
    "ACC_THRESH = 0.85\n",
    "OUTLIER_THRESH = 3.5\n",
    "WITHIN = [\"COND_interruption_condition\", \"COND_nback_level\"]\n",
    "SUBJECT = \"participant_id\"\n",
    "GROUP_COLS = [\"participant_id\", \"COND_interruption_condition\", \"COND_nback_level\"]\n",
    "ALPHA = 0.05"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "0ae81ba1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ],
   "id": "24b9f70c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def coefficient_of_variation(x):\n",
    "    \"\"\"CV = SD / Mean. Returns NaN if mean is zero.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    return np.std(x, ddof=1) / mean_x if mean_x != 0 else np.nan\n",
    "\n",
    "\n",
    "def create_condition_label(row):\n",
    "    \"\"\"Map interruption x nback factors to a short label like 'Seq-Easy'.\"\"\"\n",
    "    interruption = \"Int\" if \"interrupted\" in str(row[\"COND_interruption_condition\"]) else \"Seq\"\n",
    "    nback = \"Easy\" if row[\"COND_nback_level\"] == 1 else \"Hard\"\n",
    "    return f\"{interruption}-{nback}\"\n",
    "\n",
    "\n",
    "def robust_z(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"MAD-based robust z-score for outlier detection.\"\"\"\n",
    "    med = series.median()\n",
    "    mad = stats.median_abs_deviation(series, scale=\"normal\")\n",
    "    if mad == 0:\n",
    "        return pd.Series(np.zeros_like(series), index=series.index)\n",
    "    return 0.6745 * (series - med) / mad\n",
    "\n",
    "\n",
    "def flag_outliers_trial(df, dv, thresh=OUTLIER_THRESH):\n",
    "    \"\"\"Flag outlier trials within each participant x condition cell using robust z-score.\"\"\"\n",
    "    mask = pd.Series(False, index=df.index)\n",
    "    for _, grp in df.groupby(GROUP_COLS):\n",
    "        z = robust_z(grp[dv])\n",
    "        mask.loc[grp.index] = np.abs(z) > thresh\n",
    "    return mask\n",
    "\n",
    "\n",
    "def calculate_fit_metrics(human_data, model_data):\n",
    "    \"\"\"R-squared and RMSD between human and model condition means.\"\"\"\n",
    "    r_squared = r2_score(human_data, model_data)\n",
    "    rmsd = np.sqrt(mean_squared_error(human_data, model_data))\n",
    "    return r_squared, rmsd\n",
    "\n",
    "\n",
    "def format_mean_sd(mean_val, std_val, decimals=3):\n",
    "    \"\"\"Format as 'M (SD)' string.\"\"\"\n",
    "    return f\"{mean_val:.{decimals}f} ({std_val:.{decimals}f})\"\n",
    "\n",
    "\n",
    "def report_rm_anova(df_long, dv):\n",
    "    \"\"\"Run and print a 2-factor repeated-measures ANOVA with post-hoc tests.\"\"\"\n",
    "    aov = pg.rm_anova(\n",
    "        data=df_long, dv=dv, within=WITHIN, subject=SUBJECT, detailed=True\n",
    "    )\n",
    "    nice = (\n",
    "        aov.rename(columns={\"Source\": \"Effect\", \"ddof1\": \"df1\", \"ddof2\": \"df2\", \"p_unc\": \"p\"})\n",
    "        .loc[:, [\"Effect\", \"SS\", \"df1\", \"df2\", \"MS\", \"F\", \"p\", \"ng2\"]]\n",
    "        .round({\"SS\": 4, \"MS\": 4, \"F\": 3, \"p\": 4, \"ng2\": 3})\n",
    "    )\n",
    "    nice[\"sig\"] = np.where(nice[\"p\"] < ALPHA, \"*\", \"\")\n",
    "    print(f\"\\n=== Repeated-measures ANOVA on {dv} ===\")\n",
    "    print(nice.to_string(index=False))\n",
    "\n",
    "    ph = pg.pairwise_tests(\n",
    "        data=df_long, dv=dv, within=WITHIN, subject=SUBJECT,\n",
    "        padjust=\"holm\", parametric=True, effsize=\"hedges\",\n",
    "    )\n",
    "    sig_ph = ph[ph[\"p_corr\"] < ALPHA]\n",
    "    print(f\"\\nPost-hoc paired t-tests (Holm-corrected, \\u03b1 = .05)\")\n",
    "    if sig_ph.empty:\n",
    "        print(\"  \\u2013 none survive correction.\")\n",
    "    else:\n",
    "        keep = (\n",
    "            sig_ph[[\"A\", \"B\", \"T\", \"dof\", \"p_corr\", \"hedges\"]]\n",
    "            .rename(columns={\"A\": \"Cell A\", \"B\": \"Cell B\", \"p_corr\": \"p_Holm\", \"hedges\": \"g\"})\n",
    "            .round({\"T\": 3, \"p_Holm\": 4, \"g\": 3})\n",
    "        )\n",
    "        print(keep.to_string(index=False))\n",
    "    return aov"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "72c48836"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Task Simulations"
   ],
   "id": "8e046f3d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ],
   "id": "05d759a6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_typing_data(directory: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load all typing simulation CSVs and standardize column names.\"\"\"\n",
    "    all_files = sorted(directory.glob(\"participant_sim_*_output.csv\"))\n",
    "    print(f\"Found {len(all_files)} typing simulation files.\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"actual_duration_s\": \"OUT_actual_trial_duration_sec\",\n",
    "        \"subjective_duration_s\": \"OUT_time_estimate_seconds\",\n",
    "        \"so_ratio\": \"OUT_time_estimation_ratio\",\n",
    "        \"time_per_letter\": \"OUT_time_per_letter\",\n",
    "        \"word_length\": \"OUT_target_word_length\",\n",
    "    }\n",
    "\n",
    "    frames = []\n",
    "    for filepath in all_files:\n",
    "        try:\n",
    "            match = re.search(r\"participant_(sim_\\d+)_output\\.csv\", filepath.name)\n",
    "            if not match:\n",
    "                continue\n",
    "            df = pd.read_csv(filepath)\n",
    "            df[\"participant_id\"] = match.group(1)\n",
    "            df = df.rename(columns=rename_map)\n",
    "            # Raw so_ratio is stored as a percentage (e.g. 99.1 = 0.991)\n",
    "            df[\"OUT_time_estimation_ratio\"] = df[\"OUT_time_estimation_ratio\"] / 100.0\n",
    "            # Model produces no typing errors in single-task mode\n",
    "            df[\"OUT_typing_distance\"] = 0.0\n",
    "            frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not process {filepath.name}: {e}\")\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    print(f\"Loaded {len(combined['participant_id'].unique())} participants, {len(combined)} trials.\")\n",
    "    return combined\n",
    "\n",
    "\n",
    "def load_nback_data(directory: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load all N-back simulation CSVs and standardize column names.\"\"\"\n",
    "    all_files = sorted(directory.glob(\"participant_sim_*_*back_output.csv\"))\n",
    "    print(f\"Found {len(all_files)} N-back simulation files.\")\n",
    "\n",
    "    filename_pattern = re.compile(r\"participant_(sim_\\d+)_(1|2)back_output\\.csv\")\n",
    "\n",
    "    rename_map = {\n",
    "        \"actual_duration_s\": \"OUT_actual_trial_duration_sec\",\n",
    "        \"subjective_duration_s\": \"OUT_time_estimate_seconds\",\n",
    "        \"so_ratio\": \"OUT_time_estimation_ratio\",\n",
    "        \"accuracy\": \"OUT_nback_accuracy\",\n",
    "        \"hits\": \"OUT_nback_hits\",\n",
    "        \"misses\": \"OUT_nback_misses\",\n",
    "        \"false_alarms\": \"OUT_nback_false_alarms\",\n",
    "        \"correct_rejections\": \"OUT_nback_correct_rejections\",\n",
    "    }\n",
    "\n",
    "    frames = []\n",
    "    for filepath in all_files:\n",
    "        try:\n",
    "            match = filename_pattern.search(filepath.name)\n",
    "            if not match:\n",
    "                continue\n",
    "            pid, nback_level_str = match.groups()\n",
    "\n",
    "            df = pd.read_csv(filepath)\n",
    "            df[\"participant_id\"] = pid\n",
    "            # BUG FIX: The raw CSV has nback_level=2 in ALL files (including\n",
    "            # 1-back). The filename is the ground truth, so overwrite here.\n",
    "            df[\"nback_level\"] = int(nback_level_str)\n",
    "            df = df.rename(columns=rename_map)\n",
    "            # Raw so_ratio is stored as a percentage\n",
    "            df[\"OUT_time_estimation_ratio\"] = df[\"OUT_time_estimation_ratio\"] / 100.0\n",
    "            frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not process {filepath.name}: {e}\")\n",
    "\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    print(f\"Loaded {len(combined['participant_id'].unique())} participants, {len(combined)} trials.\")\n",
    "    return combined"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "5b34dca9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "typing_sim_df = load_typing_data(TYPING_SIM_DIR)\n",
    "nback_sim_df = load_nback_data(NBACK_SIM_DIR)\n",
    "\n",
    "print(\"\\n--- Typing simulation ---\")\n",
    "print(f\"Shape: {typing_sim_df.shape}\")\n",
    "display(typing_sim_df.head(3))\n",
    "\n",
    "print(\"\\n--- N-back simulation ---\")\n",
    "print(f\"Shape: {nback_sim_df.shape}\")\n",
    "display(nback_sim_df.head(3))\n",
    "\n",
    "# Verify repeated-measures structure: each participant should have\n",
    "# equal counts at 1-back and 2-back\n",
    "print(\"\\nTrials per participant per N-back level:\")\n",
    "print(\n",
    "    nback_sim_df.groupby(\"participant_id\")[\"nback_level\"]\n",
    "    .value_counts()\n",
    "    .unstack()\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "573df28b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ],
   "id": "56388177"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_nback_summary_table(nback_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Summary table comparing 1-back vs 2-back on performance and timing.\"\"\"\n",
    "    metric_cols = [\"OUT_nback_accuracy\", \"OUT_time_estimate_seconds\", \"OUT_time_estimation_ratio\"]\n",
    "\n",
    "    participant_means = (\n",
    "        nback_df.groupby([\"participant_id\", \"nback_level\"])[metric_cols].mean()\n",
    "    )\n",
    "\n",
    "    participant_cvs = (\n",
    "        nback_df.groupby([\"participant_id\", \"nback_level\"])[\"OUT_time_estimate_seconds\"]\n",
    "        .apply(coefficient_of_variation)\n",
    "        .rename(\"CV\")\n",
    "    )\n",
    "\n",
    "    participant_summary = pd.merge(participant_means, participant_cvs, on=[\"participant_id\", \"nback_level\"])\n",
    "    summary_stats = participant_summary.groupby(\"nback_level\").agg([\"mean\", \"std\"])\n",
    "\n",
    "    labels = {\n",
    "        \"N-back Accuracy\": \"OUT_nback_accuracy\",\n",
    "        \"Time Estimate (s)\": \"OUT_time_estimate_seconds\",\n",
    "        \"S/O Ratio\": \"OUT_time_estimation_ratio\",\n",
    "        \"CV of Time Estimates\": \"CV\",\n",
    "    }\n",
    "\n",
    "    table = pd.DataFrame(index=[1, 2])\n",
    "    table.index.name = \"N-back Level\"\n",
    "    for label, col in labels.items():\n",
    "        means = summary_stats[(col, \"mean\")]\n",
    "        stds = summary_stats[(col, \"std\")]\n",
    "        table[f\"{label} [M (SD)]\"] = [format_mean_sd(m, s) for m, s in zip(means, stds)]\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def create_typing_summary_table(typing_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Summary table for typing task performance and timing.\"\"\"\n",
    "    metric_cols = [\"OUT_time_per_letter\", \"OUT_time_estimate_seconds\", \"OUT_time_estimation_ratio\"]\n",
    "\n",
    "    participant_means = typing_df.groupby(\"participant_id\")[metric_cols].mean()\n",
    "\n",
    "    participant_cvs = (\n",
    "        typing_df.groupby(\"participant_id\")[\"OUT_time_estimate_seconds\"]\n",
    "        .apply(coefficient_of_variation)\n",
    "        .rename(\"CV\")\n",
    "    )\n",
    "\n",
    "    participant_summary = pd.merge(participant_means, participant_cvs, on=\"participant_id\")\n",
    "    summary_stats = participant_summary.agg([\"mean\", \"std\"])\n",
    "\n",
    "    labels = {\n",
    "        \"Time-per-Letter (s)\": \"OUT_time_per_letter\",\n",
    "        \"Time Estimate (s)\": \"OUT_time_estimate_seconds\",\n",
    "        \"S/O Ratio\": \"OUT_time_estimation_ratio\",\n",
    "        \"CV of Time Estimates\": \"CV\",\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for label, col in labels.items():\n",
    "        rows.append([label, format_mean_sd(summary_stats.loc[\"mean\", col], summary_stats.loc[\"std\", col])])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"Metric\", \"Value [M (SD)]\"]).set_index(\"Metric\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "e738f993"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"--- N-back Simulation Summary ---\")\n",
    "display(create_nback_summary_table(nback_sim_df))\n",
    "\n",
    "print(\"\\n--- Typing Simulation Summary ---\")\n",
    "display(create_typing_summary_table(typing_sim_df))"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "7f5eb7e3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-back Difficulty Effects"
   ],
   "id": "d64dd469"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_nback_normalized_impact(nback_df: pd.DataFrame):\n",
    "    \"\"\"Z-score normalized line plot comparing accuracy and S/O ratio across difficulty.\"\"\"\n",
    "    metrics = [\"OUT_nback_accuracy\", \"OUT_time_estimation_ratio\"]\n",
    "\n",
    "    participant_summary = (\n",
    "        nback_df.groupby([\"participant_id\", \"nback_level\"])[metrics].mean().reset_index()\n",
    "    )\n",
    "\n",
    "    for col, label in [(\"OUT_nback_accuracy\", \"Accuracy (Z-score)\"),\n",
    "                       (\"OUT_time_estimation_ratio\", \"S/O Ratio (Z-score)\")]:\n",
    "        participant_summary[label] = (\n",
    "            (participant_summary[col] - participant_summary[col].mean())\n",
    "            / participant_summary[col].std()\n",
    "        )\n",
    "\n",
    "    plot_data = pd.melt(\n",
    "        participant_summary,\n",
    "        id_vars=[\"participant_id\", \"nback_level\"],\n",
    "        value_vars=[\"Accuracy (Z-score)\", \"S/O Ratio (Z-score)\"],\n",
    "        var_name=\"Metric\",\n",
    "        value_name=\"Normalized Value (Z-score)\",\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    sns.lineplot(\n",
    "        data=plot_data,\n",
    "        x=\"nback_level\",\n",
    "        y=\"Normalized Value (Z-score)\",\n",
    "        hue=\"Metric\",\n",
    "        style=\"Metric\",\n",
    "        markers=True,\n",
    "        dashes=False,\n",
    "        palette=[\"#1f77b4\", \"#ff7f0e\"],\n",
    "        linewidth=2.5,\n",
    "        markersize=8,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.axhline(0, color=\"grey\", linestyle=\"--\", linewidth=1, alpha=0.8)\n",
    "    ax.set_title(\"Similar Impact of N-back Difficulty on Performance and Time Perception\", fontsize=18, pad=20)\n",
    "    ax.set_xlabel(\"N-back Condition\", fontsize=14, labelpad=15)\n",
    "    ax.set_ylabel(\"Normalized Value (Z-score)\", fontsize=14, labelpad=15)\n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_xticklabels([\"1-Back (Easy)\", \"2-Back (Hard)\"])\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax.legend(title=\"Metric\", fontsize=12, title_fontsize=13)\n",
    "    sns.despine(trim=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_nback_normalized_impact(nback_sim_df)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "61f7a62a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_nback_clustered_bar(nback_df: pd.DataFrame, y_bottom=None, y_top=None):\n",
    "    \"\"\"Clustered bar chart comparing accuracy and S/O ratio in absolute values.\"\"\"\n",
    "    metrics = [\"OUT_nback_accuracy\", \"OUT_time_estimation_ratio\"]\n",
    "\n",
    "    participant_summary = (\n",
    "        nback_df.groupby([\"participant_id\", \"nback_level\"])[metrics].mean().reset_index()\n",
    "    )\n",
    "\n",
    "    condition_means = (\n",
    "        participant_summary.groupby(\"nback_level\")\n",
    "        .agg(accuracy_mean=(\"OUT_nback_accuracy\", \"mean\"),\n",
    "             so_ratio_mean=(\"OUT_time_estimation_ratio\", \"mean\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    plot_data = pd.melt(\n",
    "        condition_means,\n",
    "        id_vars=\"nback_level\",\n",
    "        value_vars=[\"accuracy_mean\", \"so_ratio_mean\"],\n",
    "        var_name=\"Metric\",\n",
    "        value_name=\"Mean Value\",\n",
    "    )\n",
    "    plot_data[\"Metric\"] = plot_data[\"Metric\"].replace({\n",
    "        \"accuracy_mean\": \"N-back Accuracy\",\n",
    "        \"so_ratio_mean\": \"S/O Ratio\",\n",
    "    })\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    sns.barplot(\n",
    "        data=plot_data,\n",
    "        x=\"nback_level\",\n",
    "        y=\"Mean Value\",\n",
    "        hue=\"Metric\",\n",
    "        palette=[\"#1f77b4\", \"#ff7f0e\"],\n",
    "        edgecolor=\"black\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"Impact of N-back Difficulty on Performance and Time Perception\", fontsize=18, pad=20)\n",
    "    ax.set_xlabel(\"N-back Condition\", fontsize=14, labelpad=15)\n",
    "    ax.set_ylabel(\"Mean Value\", fontsize=14, labelpad=15)\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels([\"1-Back (Easy)\", \"2-Back (Hard)\"])\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax.legend(title=\"Metric\", fontsize=12, title_fontsize=13, loc=\"upper right\")\n",
    "    if y_bottom is not None and y_top is not None:\n",
    "        ax.set_ylim(bottom=y_bottom, top=y_top)\n",
    "    sns.despine(trim=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_nback_clustered_bar(nback_sim_df, y_bottom=0.4, y_top=1.0)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "63fd1051"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitasking Simulations"
   ],
   "id": "83841031"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ],
   "id": "0f29e772"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_simulation_data(directory, acc_thresh=ACC_THRESH):\n",
    "    \"\"\"Load simulation CSVs, return (all_data_with_labels, clean_data_for_analysis).\"\"\"\n",
    "    all_files = sorted(directory.glob(\"participant_*_CLEAN.csv\"))\n",
    "    if not all_files:\n",
    "        raise FileNotFoundError(f\"No preprocessed files in {directory}\")\n",
    "\n",
    "    frames = []\n",
    "    for filepath in all_files:\n",
    "        pid_match = re.search(\n",
    "            r\"participant_(sim_\\d+)_output_CLEAN\\.csv\", filepath.name\n",
    "        )\n",
    "        if not pid_match:\n",
    "            continue\n",
    "        df = pd.read_csv(filepath)\n",
    "        df[\"participant_id\"] = pid_match.group(1)\n",
    "        frames.append(df)\n",
    "\n",
    "    all_data = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"OUT_time_estimation_ratio\", \"OUT_normalized_absolute_error\",\n",
    "        \"OUT_time_estimate_seconds\", \"OUT_nback_accuracy\",\n",
    "        \"OUT_time_per_letter\", \"OUT_actual_trial_duration_sec\",\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in all_data.columns:\n",
    "            all_data[col] = pd.to_numeric(all_data[col], errors=\"coerce\")\n",
    "\n",
    "    all_data[\"condition_label\"] = all_data.apply(create_condition_label, axis=1)\n",
    "    all_data[\"condition_label\"] = pd.Categorical(\n",
    "        all_data[\"condition_label\"], categories=CONDITION_ORDER, ordered=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Loaded {all_data['participant_id'].nunique()} simulated participants, \"\n",
    "          f\"{len(all_data)} total trials.\")\n",
    "\n",
    "    df_main = all_data.query(\"OUT_experiment_phase == 'main'\").copy()\n",
    "    df_main = df_main[df_main[\"OUT_nback_accuracy\"] >= acc_thresh].copy()\n",
    "\n",
    "    dvs_to_screen = [\n",
    "        \"OUT_time_per_letter\",\n",
    "        \"OUT_time_estimation_ratio\",\n",
    "        \"OUT_normalized_absolute_error\",\n",
    "    ]\n",
    "    for dv in dvs_to_screen:\n",
    "        if dv in df_main.columns:\n",
    "            df_main[f\"outlier_{dv}\"] = flag_outliers_trial(df_main, dv)\n",
    "    mask_any = df_main.filter(regex=r\"^outlier_\").any(axis=1)\n",
    "    clean_data = df_main[~mask_any].copy()\n",
    "\n",
    "    n_removed = mask_any.sum()\n",
    "    print(f\"Clean subset: {len(clean_data)} trials \"\n",
    "          f\"({n_removed} outliers removed from {len(df_main)} main-phase trials).\")\n",
    "    return all_data, clean_data\n",
    "\n",
    "\n",
    "def load_human_data(directory, acc_thresh=ACC_THRESH):\n",
    "    \"\"\"Load and clean human experimental data for model comparison.\"\"\"\n",
    "    frames = []\n",
    "    for filepath in sorted(directory.glob(\"participant_*_output_CLEAN.csv\")):\n",
    "        pid_match = re.search(\n",
    "            r\"participant_(\\d+)_output_CLEAN\\.csv\", filepath.name\n",
    "        )\n",
    "        if not pid_match:\n",
    "            continue\n",
    "        df = pd.read_csv(filepath)\n",
    "        df[\"participant_id\"] = pid_match.group(1)\n",
    "        frames.append(df)\n",
    "\n",
    "    df_all = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"OUT_time_estimation_ratio\", \"OUT_normalized_absolute_error\",\n",
    "        \"OUT_time_estimate_seconds\", \"OUT_nback_accuracy\",\n",
    "        \"OUT_time_per_letter\", \"OUT_actual_trial_duration_sec\",\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df_all.columns:\n",
    "            df_all[col] = pd.to_numeric(df_all[col], errors=\"coerce\")\n",
    "\n",
    "    df_main = df_all.query(\"OUT_experiment_phase == 'main'\").copy()\n",
    "    df_main = df_main[df_main[\"OUT_nback_accuracy\"] >= acc_thresh].copy()\n",
    "\n",
    "    dvs_to_screen = [\n",
    "        \"OUT_time_estimation_ratio\",\n",
    "        \"OUT_normalized_absolute_error\",\n",
    "    ]\n",
    "    for dv in dvs_to_screen:\n",
    "        if dv in df_main.columns:\n",
    "            df_main[f\"outlier_{dv}\"] = flag_outliers_trial(df_main, dv)\n",
    "    mask_any = df_main.filter(regex=r\"^outlier_\").any(axis=1)\n",
    "    df_clean = df_main[~mask_any].copy()\n",
    "\n",
    "    print(f\"Loaded {df_all['participant_id'].nunique()} human participants, \"\n",
    "          f\"{len(df_clean)} clean trials.\")\n",
    "    return df_clean"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "124a79f2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_sim_data, df_clean_sim = load_simulation_data(SIM_DATA_DIR)\n",
    "df_clean_human = load_human_data(HUMAN_DATA_DIR)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "b632ef1d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"Simulation: {all_sim_data['participant_id'].nunique()} participants, \"\n",
    "      f\"{len(all_sim_data)} total trials, {len(df_clean_sim)} clean trials\")\n",
    "print(f\"Human: {df_clean_human['participant_id'].nunique()} participants, \"\n",
    "      f\"{len(df_clean_human)} clean trials\")\n",
    "\n",
    "sim_ids = sorted(\n",
    "    all_sim_data[\"participant_id\"].unique(),\n",
    "    key=lambda x: int(x.split(\"_\")[1]),\n",
    ")\n",
    "print(f\"\\nSimulation participants: {sim_ids}\")\n",
    "display(all_sim_data.head(3))"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "4805b3b5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Visualizations"
   ],
   "id": "80252e10"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_beeswarm_timing(data_df):\n",
    "    \"\"\"Beeswarm plot of S/O ratio across the four experimental conditions.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    sns.swarmplot(\n",
    "        x=\"condition_label\",\n",
    "        y=\"OUT_time_estimation_ratio\",\n",
    "        hue=\"condition_label\",\n",
    "        data=data_df,\n",
    "        order=CONDITION_ORDER,\n",
    "        hue_order=CONDITION_ORDER,\n",
    "        palette=[COLORS[\"teal\"], COLORS[\"teal_dark\"],\n",
    "                 COLORS[\"pink\"], COLORS[\"pink_dark\"]],\n",
    "        size=3.8,\n",
    "        alpha=0.7,\n",
    "        legend=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.axhline(1.0, color=\"orange\", linestyle=\"--\", linewidth=1.5, alpha=0.7)\n",
    "    ax.axvline(x=1.5, color=COLORS[\"primary_dark\"], linestyle=\":\",\n",
    "               alpha=0.2, linewidth=1)\n",
    "\n",
    "    ax.set_title(\"Simulated Participants: Timing Accuracy Across Conditions\",\n",
    "                 fontsize=14, fontweight=\"bold\", pad=15)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Subjective / Objective Ratio\", fontsize=12)\n",
    "    ax.set_xticks(range(len(CONDITION_ORDER)))\n",
    "    ax.set_xticklabels([\"Sequential\\nEasy\", \"Sequential\\nHard\",\n",
    "                        \"Interrupted\\nEasy\", \"Interrupted\\nHard\"])\n",
    "    ax.set_ylim(0.6, 1.1)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_beeswarm_timing(all_sim_data)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "cee72572"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_performance_bars(data_df):\n",
    "    \"\"\"Bar charts of mean trial duration and time-per-letter by condition.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    bar_palette = [\"#f3a712\", \"#e4572e\", \"#c0326e\", \"#e86af0\"]\n",
    "\n",
    "    for ax, col, ylabel, title, fmt in [\n",
    "        (axes[0], \"OUT_actual_trial_duration_sec\",\n",
    "         \"Mean Duration (seconds)\",\n",
    "         \"Mean Trial Duration by Condition\", \".2f\"),\n",
    "        (axes[1], \"OUT_time_per_letter\",\n",
    "         \"Mean Time Per Letter (seconds)\",\n",
    "         \"Mean Time Per Letter by Condition\", \".3f\"),\n",
    "    ]:\n",
    "        summary = (\n",
    "            data_df.groupby(\"condition_label\", observed=True)[col]\n",
    "            .mean()\n",
    "            .reindex(CONDITION_ORDER)\n",
    "            .reset_index()\n",
    "        )\n",
    "        bp = sns.barplot(\n",
    "            x=\"condition_label\", y=col, hue=\"condition_label\", data=summary,\n",
    "            palette=bar_palette, edgecolor=\"black\", linewidth=1.5, legend=False, ax=ax,\n",
    "        )\n",
    "        for p in bp.patches:\n",
    "            ax.annotate(\n",
    "                format(p.get_height(), fmt),\n",
    "                (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "                ha=\"center\", va=\"center\", xytext=(0, 9),\n",
    "                textcoords=\"offset points\", fontsize=11, fontweight=\"bold\",\n",
    "            )\n",
    "        ax.set_title(title, fontsize=14, pad=15)\n",
    "        ax.set_xlabel(\"Condition\", fontsize=12)\n",
    "        ax.set_ylabel(ylabel, fontsize=12)\n",
    "        min_val = summary[col].min()\n",
    "        ax.set_ylim(bottom=min_val * 0.95)\n",
    "        sns.despine(left=True)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_performance_bars(all_sim_data)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "026ffc33"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_nback_accuracy_by_level(data_df):\n",
    "    \"\"\"Bar chart of mean N-back accuracy: 1-back vs 2-back.\"\"\"\n",
    "    data_df = data_df.copy()\n",
    "    data_df[\"nback_accuracy_percent\"] = data_df[\"OUT_nback_accuracy\"] * 100\n",
    "\n",
    "    summary = (\n",
    "        data_df.groupby(\"COND_nback_level\")[\"nback_accuracy_percent\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary[\"label\"] = summary[\"COND_nback_level\"].apply(lambda x: f\"{x}-Back\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bp = sns.barplot(\n",
    "        x=\"label\", y=\"nback_accuracy_percent\", hue=\"label\", data=summary,\n",
    "        palette=[\"#f3a712\", \"#e4572e\"], edgecolor=\"black\", linewidth=1.5, legend=False, ax=ax,\n",
    "    )\n",
    "    for p in bp.patches:\n",
    "        ax.annotate(\n",
    "            f\"{p.get_height():.2f}%\",\n",
    "            (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "            ha=\"center\", va=\"center\", xytext=(0, 9),\n",
    "            textcoords=\"offset points\", fontsize=12, fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_title(\"Mean N-Back Accuracy by Task Difficulty\", fontsize=14, pad=15)\n",
    "    ax.set_xlabel(\"N-Back Level\", fontsize=12)\n",
    "    ax.set_ylabel(\"Mean Accuracy (%)\", fontsize=12)\n",
    "    min_val = summary[\"nback_accuracy_percent\"].min()\n",
    "    max_val = summary[\"nback_accuracy_percent\"].max()\n",
    "    ax.set_ylim(bottom=min_val * 0.3, top=max_val * 1.05)\n",
    "    sns.despine(left=True)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_nback_accuracy_by_level(all_sim_data)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "75309b3a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_time_perception_by_nback(data_df):\n",
    "    \"\"\"Absolute error and S/O ratio by N-back level (1x2 subplot).\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    palette = [\"#f3a712\", \"#e4572e\"]\n",
    "\n",
    "    for ax, col, ylabel, title, fmt in [\n",
    "        (axes[0], \"OUT_normalized_absolute_error\",\n",
    "         \"Mean Normalized Absolute Error\",\n",
    "         \"Absolute Error by Task Difficulty\", \".3f\"),\n",
    "        (axes[1], \"OUT_time_estimation_ratio\",\n",
    "         \"Mean S/O Ratio\",\n",
    "         \"S/O Ratio by Task Difficulty\", \".3f\"),\n",
    "    ]:\n",
    "        by_condition = (\n",
    "            data_df.groupby(\"condition_label\", observed=True)[col]\n",
    "            .mean().reset_index()\n",
    "        )\n",
    "        easy_mean = by_condition[\n",
    "            by_condition[\"condition_label\"].isin([\"Seq-Easy\", \"Int-Easy\"])\n",
    "        ][col].mean()\n",
    "        hard_mean = by_condition[\n",
    "            by_condition[\"condition_label\"].isin([\"Seq-Hard\", \"Int-Hard\"])\n",
    "        ][col].mean()\n",
    "\n",
    "        summary = pd.DataFrame({\n",
    "            \"N-Back Level\": [\"1-Back (Easy)\", \"2-Back (Hard)\"],\n",
    "            \"value\": [easy_mean, hard_mean],\n",
    "        })\n",
    "\n",
    "        bp = sns.barplot(\n",
    "            x=\"N-Back Level\", y=\"value\", hue=\"N-Back Level\", data=summary,\n",
    "            palette=palette, edgecolor=\"black\", linewidth=1.5, legend=False, ax=ax,\n",
    "        )\n",
    "        for p in bp.patches:\n",
    "            ax.annotate(\n",
    "                format(p.get_height(), fmt),\n",
    "                (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "                ha=\"center\", va=\"center\", xytext=(0, 9),\n",
    "                textcoords=\"offset points\", fontsize=12, fontweight=\"bold\",\n",
    "            )\n",
    "        ax.set_title(title, fontsize=14, pad=15)\n",
    "        ax.set_xlabel(\"N-Back Level\", fontsize=12)\n",
    "        ax.set_ylabel(ylabel, fontsize=12)\n",
    "        if col == \"OUT_normalized_absolute_error\":\n",
    "            ax.set_ylim(bottom=0, top=summary[\"value\"].max() * 1.15)\n",
    "        else:\n",
    "            ax.set_ylim(bottom=summary[\"value\"].min() * 0.95, top=1.05)\n",
    "            ax.axhline(1.0, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "        sns.despine(left=True)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_time_perception_by_nback(all_sim_data)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "08cbabd2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_metrics_by_interruption(data_df):\n",
    "    \"\"\"S/O ratio, absolute error, CV, and trial duration by interruption condition (2x2).\"\"\"\n",
    "    int_palette = [\"#c0326e\", \"#e86af0\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # S/O Ratio\n",
    "    ax = axes[0, 0]\n",
    "    s = data_df.groupby(\"COND_interruption_condition\")[\"OUT_time_estimation_ratio\"].mean().reset_index()\n",
    "    bp = sns.barplot(x=\"COND_interruption_condition\", y=\"OUT_time_estimation_ratio\",\n",
    "                     hue=\"COND_interruption_condition\", data=s,\n",
    "                     palette=int_palette, edgecolor=\"black\", linewidth=1.5, legend=False, ax=ax)\n",
    "    for p in bp.patches:\n",
    "        ax.annotate(f\"{p.get_height():.3f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha=\"center\", va=\"center\", xytext=(0, 9), textcoords=\"offset points\",\n",
    "                    fontsize=11, fontweight=\"bold\")\n",
    "    ax.axhline(1.0, color=\"black\", linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "    ax.set_title(\"Mean S/O Ratio by Interruption\", fontsize=13, pad=10)\n",
    "    ax.set_xlabel(\"\"); ax.set_ylabel(\"S/O Ratio\", fontsize=11)\n",
    "    ax.set_ylim(bottom=s[\"OUT_time_estimation_ratio\"].min() * 0.95, top=1.05)\n",
    "\n",
    "    # Absolute Error\n",
    "    ax = axes[0, 1]\n",
    "    s = data_df.groupby(\"COND_interruption_condition\")[\"OUT_normalized_absolute_error\"].mean().reset_index()\n",
    "    bp = sns.barplot(x=\"COND_interruption_condition\", y=\"OUT_normalized_absolute_error\",\n",
    "                     hue=\"COND_interruption_condition\", data=s,\n",
    "                     palette=int_palette, edgecolor=\"black\", linewidth=1.5, legend=False, ax=ax)\n",
    "    for p in bp.patches:\n",
    "        ax.annotate(f\"{p.get_height():.3f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha=\"center\", va=\"center\", xytext=(0, 9), textcoords=\"offset points\",\n",
    "                    fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_title(\"Mean Absolute Error by Interruption\", fontsize=13, pad=10)\n",
    "    ax.set_xlabel(\"\"); ax.set_ylabel(\"Absolute Error\", fontsize=11)\n",
    "    ax.set_ylim(bottom=0, top=s[\"OUT_normalized_absolute_error\"].max() * 1.15)\n",
    "\n",
    "    # CV\n",
    "    ax = axes[1, 0]\n",
    "    cv_per_part = (\n",
    "        data_df.groupby([\"participant_id\", \"condition_label\"], observed=True)[\"OUT_time_estimate_seconds\"]\n",
    "        .agg(sd=\"std\", mean=\"mean\").reset_index()\n",
    "    )\n",
    "    cv_per_part[\"cv\"] = (cv_per_part[\"sd\"] / cv_per_part[\"mean\"]).fillna(0)\n",
    "    cv_per_part[\"interruption\"] = cv_per_part[\"condition_label\"].apply(\n",
    "        lambda x: \"interrupted\" if \"Int\" in x else \"sequential\"\n",
    "    )\n",
    "    s = cv_per_part.groupby(\"interruption\")[\"cv\"].mean().reset_index()\n",
    "    bp = sns.barplot(x=\"interruption\", y=\"cv\", hue=\"interruption\", data=s,\n",
    "                     palette=int_palette, edgecolor=\"black\", linewidth=1.5, legend=False, ax=ax)\n",
    "    for p in bp.patches:\n",
    "        ax.annotate(f\"{p.get_height():.3f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha=\"center\", va=\"center\", xytext=(0, 9), textcoords=\"offset points\",\n",
    "                    fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_title(\"Mean CV by Interruption\", fontsize=13, pad=10)\n",
    "    ax.set_xlabel(\"\"); ax.set_ylabel(\"Coefficient of Variation\", fontsize=11)\n",
    "    ax.set_ylim(bottom=0.16, top=s[\"cv\"].max() * 1.05)\n",
    "\n",
    "    # Trial Duration\n",
    "    ax = axes[1, 1]\n",
    "    s = data_df.groupby(\"COND_interruption_condition\")[\"OUT_actual_trial_duration_sec\"].mean().reset_index()\n",
    "    bp = sns.barplot(x=\"COND_interruption_condition\", y=\"OUT_actual_trial_duration_sec\",\n",
    "                     hue=\"COND_interruption_condition\", data=s,\n",
    "                     palette=int_palette, edgecolor=\"black\", linewidth=1.5, legend=False, ax=ax)\n",
    "    for p in bp.patches:\n",
    "        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha=\"center\", va=\"center\", xytext=(0, 9), textcoords=\"offset points\",\n",
    "                    fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_title(\"Mean Trial Duration by Interruption\", fontsize=13, pad=10)\n",
    "    ax.set_xlabel(\"\"); ax.set_ylabel(\"Duration (seconds)\", fontsize=11)\n",
    "    min_val = s[\"OUT_actual_trial_duration_sec\"].min()\n",
    "    max_val = s[\"OUT_actual_trial_duration_sec\"].max()\n",
    "    ax.set_ylim(bottom=min_val * 0.95, top=max_val * 1.05)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        sns.despine(left=True, ax=ax)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.suptitle(\"Main Effects of Interruption Condition\",\n",
    "                 fontsize=15, fontweight=\"bold\", y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_metrics_by_interruption(all_sim_data)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "2992749b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ],
   "id": "facd52fd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "TP_METRIC_COLS = [\"mean_SO_ratio\", \"mean_abs_error\", \"cv_estimate_seconds\"]\n",
    "\n",
    "\n",
    "def aggregate_time_perception(df_clean):\n",
    "    \"\"\"Per-participant means, then grand means per condition.\"\"\"\n",
    "    part_means = df_clean.groupby(GROUP_COLS, as_index=False).agg(\n",
    "        mean_SO_ratio=(\"OUT_time_estimation_ratio\", \"mean\"),\n",
    "        mean_abs_error=(\"OUT_normalized_absolute_error\", \"mean\"),\n",
    "        cv_estimate_seconds=(\"OUT_time_estimate_seconds\", coefficient_of_variation),\n",
    "    )\n",
    "    summary = (\n",
    "        part_means\n",
    "        .groupby([\"COND_interruption_condition\", \"COND_nback_level\"])\n",
    "        [TP_METRIC_COLS].mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    return part_means, summary\n",
    "\n",
    "\n",
    "human_tp_part, human_tp_summary = aggregate_time_perception(df_clean_human)\n",
    "model_tp_part, model_tp_summary = aggregate_time_perception(df_clean_sim)\n",
    "\n",
    "print(\"--- Human Time Perception Summary ---\")\n",
    "display(human_tp_summary)\n",
    "print(\"\\n--- Model Time Perception Summary ---\")\n",
    "display(model_tp_summary)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "c273ee7e"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def aggregate_by_conditions(df, col):\n",
    "    \"\"\"2-step aggregation: per-participant means -> grand means per condition.\"\"\"\n",
    "    part = df.groupby(GROUP_COLS)[col].mean().reset_index()\n",
    "    return (\n",
    "        part.groupby([\"COND_interruption_condition\", \"COND_nback_level\"])\n",
    "        [col].mean().reset_index()\n",
    "    )\n",
    "\n",
    "def aggregate_by_nback(df, col):\n",
    "    \"\"\"2-step aggregation by nback level only.\"\"\"\n",
    "    part = (\n",
    "        df.groupby([\"participant_id\", \"COND_nback_level\"])[col]\n",
    "        .mean().reset_index()\n",
    "    )\n",
    "    return part.groupby(\"COND_nback_level\")[col].mean().reset_index()\n",
    "\n",
    "\n",
    "# N-back accuracy\n",
    "human_nback_summary = aggregate_by_nback(df_clean_human, \"OUT_nback_accuracy\")\n",
    "model_nback_summary = aggregate_by_nback(df_clean_sim, \"OUT_nback_accuracy\")\n",
    "\n",
    "# Trial duration\n",
    "human_duration_summary = aggregate_by_conditions(df_clean_human, \"OUT_actual_trial_duration_sec\")\n",
    "model_duration_summary = aggregate_by_conditions(df_clean_sim, \"OUT_actual_trial_duration_sec\")\n",
    "\n",
    "# Time per letter\n",
    "human_tpl_summary = aggregate_by_conditions(df_clean_human, \"OUT_time_per_letter\")\n",
    "model_tpl_summary = aggregate_by_conditions(df_clean_sim, \"OUT_time_per_letter\")\n",
    "\n",
    "print(\"--- N-Back Accuracy ---\")\n",
    "print(pd.merge(human_nback_summary, model_nback_summary,\n",
    "               on=\"COND_nback_level\", suffixes=(\"_human\", \"_model\")))\n",
    "\n",
    "print(\"\\n--- Trial Duration ---\")\n",
    "print(pd.merge(human_duration_summary, model_duration_summary,\n",
    "               on=[\"COND_interruption_condition\", \"COND_nback_level\"],\n",
    "               suffixes=(\"_human\", \"_model\")))\n",
    "\n",
    "print(\"\\n--- Time Per Letter ---\")\n",
    "print(pd.merge(human_tpl_summary, model_tpl_summary,\n",
    "               on=[\"COND_interruption_condition\", \"COND_nback_level\"],\n",
    "               suffixes=(\"_human\", \"_model\")))"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "60495be4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Compute R\\u00b2 and RMSD for all metrics ---\n",
    "fit_results = []\n",
    "\n",
    "# Time perception metrics\n",
    "tp_fit_df = pd.merge(\n",
    "    human_tp_summary, model_tp_summary,\n",
    "    on=[\"COND_interruption_condition\", \"COND_nback_level\"],\n",
    "    suffixes=(\"_human\", \"_model\"),\n",
    ")\n",
    "\n",
    "for metric, label in [\n",
    "    (\"mean_SO_ratio\", \"SO_ratio\"),\n",
    "    (\"mean_abs_error\", \"absolute_error\"),\n",
    "    (\"cv_estimate_seconds\", \"CV\"),\n",
    "]:\n",
    "    r2, rmsd = calculate_fit_metrics(\n",
    "        tp_fit_df[f\"{metric}_human\"], tp_fit_df[f\"{metric}_model\"]\n",
    "    )\n",
    "    fit_results.append({\"Metric\": label, \"R_squared\": r2, \"RMSD\": rmsd})\n",
    "\n",
    "# N-back accuracy\n",
    "nback_fit = pd.merge(\n",
    "    human_nback_summary, model_nback_summary,\n",
    "    on=\"COND_nback_level\", suffixes=(\"_human\", \"_model\"),\n",
    ")\n",
    "r2, rmsd = calculate_fit_metrics(\n",
    "    nback_fit[\"OUT_nback_accuracy_human\"], nback_fit[\"OUT_nback_accuracy_model\"]\n",
    ")\n",
    "fit_results.append({\"Metric\": \"nback_accuracy\", \"R_squared\": r2, \"RMSD\": rmsd})\n",
    "\n",
    "# Trial duration\n",
    "dur_fit = pd.merge(\n",
    "    human_duration_summary, model_duration_summary,\n",
    "    on=[\"COND_interruption_condition\", \"COND_nback_level\"],\n",
    "    suffixes=(\"_human\", \"_model\"),\n",
    ")\n",
    "r2, rmsd = calculate_fit_metrics(\n",
    "    dur_fit[\"OUT_actual_trial_duration_sec_human\"],\n",
    "    dur_fit[\"OUT_actual_trial_duration_sec_model\"],\n",
    ")\n",
    "fit_results.append({\"Metric\": \"trial_duration\", \"R_squared\": r2, \"RMSD\": rmsd})\n",
    "\n",
    "# Time per letter\n",
    "tpl_fit = pd.merge(\n",
    "    human_tpl_summary, model_tpl_summary,\n",
    "    on=[\"COND_interruption_condition\", \"COND_nback_level\"],\n",
    "    suffixes=(\"_human\", \"_model\"),\n",
    ")\n",
    "r2, rmsd = calculate_fit_metrics(\n",
    "    tpl_fit[\"OUT_time_per_letter_human\"],\n",
    "    tpl_fit[\"OUT_time_per_letter_model\"],\n",
    ")\n",
    "fit_results.append({\"Metric\": \"time_per_letter\", \"R_squared\": r2, \"RMSD\": rmsd})\n",
    "\n",
    "# Display\n",
    "fit_results_df = pd.DataFrame(fit_results).set_index(\"Metric\")\n",
    "fit_results_df[\"R_squared\"] = fit_results_df[\"R_squared\"].round(2)\n",
    "fit_results_df[\"RMSD\"] = fit_results_df[\"RMSD\"].round(3)\n",
    "print(\"Model Fit Results (R\\u00b2 and RMSD)\")\n",
    "display(fit_results_df)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "b7271619"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_model_fit_grid():\n",
    "    \"\"\"2\\u00d73 grid comparing human vs model across all six metrics.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    plot_configs = [\n",
    "        {\"data\": (human_tp_summary, model_tp_summary), \"metric\": \"mean_abs_error\",\n",
    "         \"title\": \"Absolute Error\", \"ylabel\": \"Absolute Error\", \"ylim\": (0.03, 0.20)},\n",
    "        {\"data\": (human_tp_summary, model_tp_summary), \"metric\": \"mean_SO_ratio\",\n",
    "         \"title\": \"S/O Ratio\", \"ylabel\": \"Subjective/Objective Ratio\", \"ylim\": (0.7, 1.05)},\n",
    "        {\"data\": (human_tp_summary, model_tp_summary), \"metric\": \"cv_estimate_seconds\",\n",
    "         \"title\": \"Coefficient of Variation\", \"ylabel\": \"CV\", \"ylim\": (0.08, 0.20)},\n",
    "        {\"data\": (human_nback_summary, model_nback_summary), \"metric\": \"OUT_nback_accuracy\",\n",
    "         \"title\": \"N-back Accuracy\", \"ylabel\": \"Accuracy\", \"ylim\": (0.98, 1.0),\n",
    "         \"is_nback\": True},\n",
    "        {\"data\": (human_duration_summary, model_duration_summary),\n",
    "         \"metric\": \"OUT_actual_trial_duration_sec\",\n",
    "         \"title\": \"Trial Duration\", \"ylabel\": \"Duration (s)\", \"ylim\": (20, 25)},\n",
    "        {\"data\": (human_tpl_summary, model_tpl_summary), \"metric\": \"OUT_time_per_letter\",\n",
    "         \"title\": \"Time per Letter\", \"ylabel\": \"Time per Letter (s)\", \"ylim\": (0.8, 2)},\n",
    "    ]\n",
    "\n",
    "    for ax, config in zip(axes, plot_configs):\n",
    "        human_data, model_data = config[\"data\"]\n",
    "        metric = config[\"metric\"]\n",
    "\n",
    "        if config.get(\"is_nback\", False):\n",
    "            x_pos = [1, 2]\n",
    "            x_labels = [\"1-back\", \"2-back\"]\n",
    "            human_vals = human_data.sort_values(\"COND_nback_level\")[metric].values\n",
    "            model_vals = model_data.sort_values(\"COND_nback_level\")[metric].values\n",
    "        else:\n",
    "            x_pos = [1, 2, 3, 4]\n",
    "            x_labels = [\"Seq\\nEasy\", \"Seq\\nHard\", \"Int\\nEasy\", \"Int\\nHard\"]\n",
    "            h_sorted = human_data.sort_values([\"COND_interruption_condition\", \"COND_nback_level\"])\n",
    "            m_sorted = model_data.sort_values([\"COND_interruption_condition\", \"COND_nback_level\"])\n",
    "            human_vals = np.concatenate([\n",
    "                h_sorted[h_sorted[\"COND_interruption_condition\"] == \"sequential\"][metric].values,\n",
    "                h_sorted[h_sorted[\"COND_interruption_condition\"] == \"interrupted\"][metric].values,\n",
    "            ])\n",
    "            model_vals = np.concatenate([\n",
    "                m_sorted[m_sorted[\"COND_interruption_condition\"] == \"sequential\"][metric].values,\n",
    "                m_sorted[m_sorted[\"COND_interruption_condition\"] == \"interrupted\"][metric].values,\n",
    "            ])\n",
    "\n",
    "        ax.plot(x_pos, human_vals, \"o-\", color=COLORS[\"teal_dark\"], linewidth=2,\n",
    "                markersize=8, markeredgecolor=\"white\", markeredgewidth=1)\n",
    "        ax.plot(x_pos, model_vals, \"s--\", color=COLORS[\"pink_dark\"], linewidth=2,\n",
    "                markersize=7, markeredgecolor=\"white\", markeredgewidth=1)\n",
    "\n",
    "        if metric == \"mean_SO_ratio\":\n",
    "            ax.axhline(y=1.0, color=COLORS[\"primary_dark\"], linestyle=\":\", alpha=0.3)\n",
    "\n",
    "        ax.set_title(config[\"title\"], fontsize=11, fontweight=\"bold\", pad=5)\n",
    "        ax.set_ylabel(config[\"ylabel\"], fontsize=9)\n",
    "        ax.set_ylim(config[\"ylim\"])\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(x_labels, fontsize=8)\n",
    "        ax.tick_params(axis=\"y\", labelsize=8)\n",
    "        ax.yaxis.grid(True, alpha=0.15, linestyle=\"--\")\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "        if not config.get(\"is_nback\", False):\n",
    "            ax.axvline(x=2.5, color=COLORS[\"primary_dark\"], linestyle=\":\", alpha=0.2)\n",
    "\n",
    "    fig.suptitle(\"Model Fit: Comparison Across Metrics\",\n",
    "                 fontsize=15, fontweight=\"bold\", y=1.02)\n",
    "\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=COLORS[\"teal_dark\"], linestyle=\"-\", marker=\"o\",\n",
    "               markersize=8, label=\"Human\", markeredgecolor=\"white\", markeredgewidth=1),\n",
    "        Line2D([0], [0], color=COLORS[\"pink_dark\"], linestyle=\"--\", marker=\"s\",\n",
    "               markersize=7, label=\"Model\", markeredgecolor=\"white\", markeredgewidth=1),\n",
    "    ]\n",
    "    fig.legend(handles=legend_elements, loc=\"upper center\",\n",
    "               bbox_to_anchor=(0.5, 0.98), ncol=2, frameon=True,\n",
    "               fontsize=10, columnspacing=3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88, bottom=0.05, left=0.08, right=0.98)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_model_fit_grid()"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "898e3198"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\\u00d72 Repeated-Measures ANOVAs"
   ],
   "id": "1d4c3c0d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "agg_perf_sim = df_clean_sim.groupby(GROUP_COLS, as_index=False).agg(\n",
    "    mean_time_per_letter=(\"OUT_time_per_letter\", \"mean\"),\n",
    ")\n",
    "\n",
    "agg_time_sim = df_clean_sim.groupby(GROUP_COLS, as_index=False).agg(\n",
    "    mean_SO_ratio=(\"OUT_time_estimation_ratio\", \"mean\"),\n",
    "    mean_abs_error=(\"OUT_normalized_absolute_error\", \"mean\"),\n",
    "    cv_estimate_seconds=(\"OUT_time_estimate_seconds\", coefficient_of_variation),\n",
    ")\n",
    "\n",
    "print(f\"Performance aggregation: {len(agg_perf_sim)} rows \"\n",
    "      f\"({agg_perf_sim['participant_id'].nunique()} participants)\")\n",
    "print(f\"Time perception aggregation: {len(agg_time_sim)} rows\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "e0541e8a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "report_rm_anova(agg_perf_sim, \"mean_time_per_letter\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "5dfa42bc"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for dv in [\"mean_SO_ratio\", \"mean_abs_error\", \"cv_estimate_seconds\"]:\n",
    "    report_rm_anova(agg_time_sim, dv)"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "21cd4da9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_interaction_panel(agg_df, metrics, ylabels, titles, suptitle):\n",
    "    \"\"\"Grouped bar chart interaction plot panel (matches HIGH RES original).\"\"\"\n",
    "    n = len(metrics)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(5 * n, 5))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (ax, metric, ylabel, title) in enumerate(zip(axes, metrics, ylabels, titles)):\n",
    "        means = agg_df.groupby(['COND_nback_level', 'COND_interruption_condition'])[metric].mean().unstack()\n",
    "        sems = agg_df.groupby(['COND_nback_level', 'COND_interruption_condition'])[metric].sem().unstack()\n",
    "\n",
    "        x = np.arange(2)\n",
    "        width = 0.35\n",
    "\n",
    "        ax.bar(x - width/2, means['sequential'].values, width,\n",
    "               yerr=sems['sequential'].values, capsize=5,\n",
    "               color=COLORS['teal'], edgecolor=COLORS['teal_dark'], linewidth=1.5,\n",
    "               label='Sequential', alpha=0.8)\n",
    "\n",
    "        ax.bar(x + width/2, means['interrupted'].values, width,\n",
    "               yerr=sems['interrupted'].values, capsize=5,\n",
    "               color=COLORS['pink'], edgecolor=COLORS['pink_dark'], linewidth=1.5,\n",
    "               label='Interrupted', alpha=0.8)\n",
    "\n",
    "        ax.set_xlabel('Task Complexity', fontsize=11)\n",
    "        ax.set_ylabel(ylabel, fontsize=11)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(['1-back\\n(Easy)', '2-back\\n(Hard)'])\n",
    "\n",
    "        if metric == 'mean_SO_ratio':\n",
    "            ax.axhline(y=1.0, color=COLORS['primary_dark'], linestyle=':', alpha=0.3)\n",
    "            ax.set_ylim(0.76, 0.97)\n",
    "\n",
    "        ax.yaxis.grid(True, alpha=0.15, linestyle='--')\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        if idx == 0:\n",
    "            ax.legend(loc='upper left', frameon=True)\n",
    "\n",
    "    fig.suptitle(suptitle, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_interaction_panel(\n",
    "    agg_time_sim,\n",
    "    metrics=[\"mean_abs_error\", \"mean_SO_ratio\"],\n",
    "    ylabels=[\"Absolute Error\", \"Subjective/Objective Ratio\"],\n",
    "    titles=[\"Absolute Error\", \"S/O Ratio\"],\n",
    "    suptitle=\"Model Predictions: Interaction Effect on Time Perception\",\n",
    ")\n",
    "\n",
    "plot_interaction_panel(\n",
    "    agg_perf_sim,\n",
    "    metrics=[\"mean_time_per_letter\"],\n",
    "    ylabels=[\"Mean Time per Letter (s)\"],\n",
    "    titles=[\"Time per Letter\"],\n",
    "    suptitle=\"Model Predictions: Interaction Effect on Task Performance\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "10e3a72f"
  }
 ]
}